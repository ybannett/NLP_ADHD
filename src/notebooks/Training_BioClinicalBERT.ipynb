{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_curve, RocCurveDisplay\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import sklearn\n",
    "import os\n",
    "import shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path for importing required functions for data processing\n",
    "\n",
    "sys.path.append(\"data_processing\")\n",
    "sys.path.append(\"data_processing\")\n",
    "\n",
    "# Functions required for data processing\n",
    "\n",
    "import process_text\n",
    "import transform_textfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859fd35",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd6eaf",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05111cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractOriginalText2(input_path):\n",
    "    # read in progress note from txt file (same txt file that was imported to CLAMP for annotation)\n",
    "    results = glob.glob(f\"{input_path}/*.txt\")\n",
    "    rows = []\n",
    "    \n",
    "    # extract original note text from the progress note and save as dataframe\n",
    "    for i in range(len(results)):\n",
    "        row = pd.read_csv(results[i], sep=\"\\t\", quoting=csv.QUOTE_NONE).iloc[:, [6]]\n",
    "        row.columns = ['note_des']\n",
    "        row['file'] = os.path.basename(results[i]).split(\".\")[0]\n",
    "        rows.append(row)\n",
    "    return pd.concat(rows)\n",
    "\n",
    "def extractXMIAnnotation2(input_path):\n",
    "    # write Patterns\n",
    "    id_match = re.compile(\"(?<=xmi:id=\\\")\\d{,5}(?=\\\")\")\n",
    "    begin_match = re.compile(\"(?<=begin=\\\")\\d{,6}(?=\\\")\")\n",
    "    end_match = re.compile(\"(?<=end=\\\")\\d{,6}(?=\\\")\")\n",
    "    tag_match = re.compile(\"(?<=semanticTag=\\\")\\w*(?=\\\")\")\n",
    "\n",
    "    # read annotation file (notes were annotated in CLAMP)\n",
    "    # annotation files were exported from CLAMP in XMI format\n",
    "    results = glob.glob(f\"{input_path}/*.xmi\")\n",
    "    rows = []\n",
    "    \n",
    "    # extract annotations from XMI files (noted with semanticTag field)\n",
    "    for i in range(len(results)):\n",
    "        file = open(results[i], \"r+\")\n",
    "        lines = file.readlines()\n",
    "        lines = lines[0].split('><')\n",
    "        extract = [x for x in lines if re.search(\"semanticTag\", x)]\n",
    "        extract = [(id_match.findall(x),\n",
    "                    begin_match.findall(x),\n",
    "                    end_match.findall(x),\n",
    "                    tag_match.findall(x))\n",
    "                   for x in extract]\n",
    "        unique_tags = sorted(list(set([x[3][0] for x in extract])))\n",
    "#         file_name = re.findall(\"\\\\A.*(?=\\.)\",os.path.basename(results[i]))[0].split(\"-\")\n",
    "        file_name = os.path.basename(results[i]).split(\".\")[0]\n",
    "        \n",
    "        row = pd.DataFrame({'xmi': [os.path.basename(results[i])],\n",
    "                            'file': [os.path.basename(results[i]).split(\".\")[0]],\n",
    "                            'anon_id': [file_name.split(\"-\")[1]],\n",
    "                            'encounter_id': [file_name.split(\"-\")[2]]})\n",
    "\n",
    "        for tag in unique_tags:\n",
    "            row.loc[:, tag] = 1\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    full = pd.concat(rows).fillna(0)\n",
    "\n",
    "    # categorize and process types of ground truth annotations\n",
    "    ## PTBM: parent training in behavioral management\n",
    "    ## weak_bt: weak evidence of PTBM\n",
    "    ## strong_bt: strong evidence of PTBM\n",
    "    ## bt_yn: binary variable for any evidence of PTBM\n",
    "    full['weak_bt'] = np.where((full['Counsel_Handout_BT'] == 1) |\n",
    "                               (full['Counsel_Parent_BT'] == 1), 1, 0)\n",
    "\n",
    "    full['strong_bt'] = np.where((full['Refer_Parent_BT'] == 1) |\n",
    "                                 (full['Refer_School_BT'] == 1), 1, 0)\n",
    "\n",
    "#     full['bt_yn'] = np.where((full['weak_bt'] == 1) |\n",
    "#                              (full['strong_bt'] == 1), 1, 0)\n",
    "\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get required structured data for model in dict\n",
    "## Structure\n",
    "## tabular_data = pd.DataFrame({\n",
    "##     'dis_symp': [0, 1, 1], # 0 = disorder-level code; 1 = symptom-level code\n",
    "##     'age_35_6': [1, 0, 1], # 0 = 3-5 years old; 1 = 6 years old\n",
    "## })\n",
    "# the required variables are hard-coded into the function - make sure to change if needed\n",
    "# patient ID is set as ANON_ID\n",
    "\n",
    "def get_tabular_data(sdata, X_set):\n",
    "    tab = pd.merge(sdata, X_set, left_on='ANON_ID', right_on=X_set.index, how='right')\n",
    "    tab['dis_symp'] = tab['only.symp']\n",
    "    tab['age_35_6'] = tab['ADHD.age'].apply(lambda x: 0 if x < 6 else 1)\n",
    "    tab = tab.set_index('ANON_ID')\n",
    "    tab = tab[['dis_symp', 'age_35_6']].to_dict('records')\n",
    "    \n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataset with note text, structured data, and note labels\n",
    "def get_dataset(text_data, tabular_data, labels):\n",
    "    dataset = [\n",
    "        {\n",
    "            'input_ids': text_data['input_ids'][idx],\n",
    "            'attention_mask': text_data['attention_mask'][idx],\n",
    "            'tabular_data': torch.tensor(list(sample.values()), dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        for idx, sample in enumerate(tabular_data)\n",
    "    ]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173bf6bc",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set metrics for comparison\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=1)\n",
    "    probabilities = torch.sigmoid(torch.tensor(p.predictions)).cpu().numpy()\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    roc_auc = roc_auc_score(labels, probabilities[:, 1])\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-rocket",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_of_interest = \"BT_yn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull and process original text data (takes in a directory of progress note txt files)\n",
    "originalTextData = extractOriginalText2(\"cohort_2to6/Text files/combined_text\")\n",
    "print(originalTextData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull and process annotations (takes in a directory of CLAMP outputted annotation XMI files)\n",
    "annotatedXMIs = extractXMIAnnotation2(\"cohort_2to6/XMI files/combined\")\n",
    "print(annotatedXMIs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set binary outcome variable\n",
    "\n",
    "annotatedXMIs['BT_yn'] = np.where((annotatedXMIs['Counsel_Parent_BT'] == 1) | (annotatedXMIs['Counsel_Handout_BT'] == 1) | (annotatedXMIs['Refer_Parent_BT'] == 1) | (annotatedXMIs['Refer_School_BT'] == 1), 1, 0)\n",
    "annotatedXMIs['BT_yn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging data from both files \n",
    "data = originalTextData.merge(annotatedXMIs, on = \"file\", how = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function sectionize() from process_text for processing notes text data\n",
    "\n",
    "data['extractText'] = data['note_des'].apply(lambda x: process_text.sectionize(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function clean_text() for processing notes text data\n",
    "\n",
    "data['extractText'] = data['extractText'].apply(lambda x: process_text.clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in structured data (not included in repository due to PHI)\n",
    "structured_data = pd.read_csv(\"bt_demographics.csv\")\n",
    "# structured_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set patient ID type to int\n",
    "data.anon_id = data.anon_id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge text data with structured data to get complete dataset\n",
    "data = pd.merge(structured_data, data, left_on='ANON_ID', right_on='anon_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter down columns in dataset to those necessary for analysis:\n",
    "## text and label\n",
    "data = data.loc[:, ['extractText',label_of_interest, 'ANON_ID']]\\\n",
    "       .rename(columns = {'extractText':'text',\n",
    "                          label_of_interest: 'label'})\n",
    "\n",
    "data = data.set_index('ANON_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, 'text']\n",
    "y = data.loc[:, 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-timer",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validation, and test sets (stratified)\n",
    "## 70/30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 117, stratify = y)\n",
    "X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_train, y_train, test_size = 0.3, random_state = 117, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_train = pd.concat([X_val_train, y_val_train], axis = 1)\n",
    "val_test = pd.concat([X_val_test, y_val_test], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-acting",
   "metadata": {},
   "source": [
    " #Checking the final size for train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: \", X_val_train.shape)\n",
    "print(\"X_val shape: \", X_val_test.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome distribution in train set\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome distribution in validation set\n",
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome distribution in test set\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-template",
   "metadata": {},
   "source": [
    "# Get Dataset for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce structured dataframe to only include the selected variables\n",
    "red_sdata = structured_data[['ANON_ID', 'ever.dis', 'only.symp', 'ADHD.age']]\n",
    "\n",
    "# train set tabular data\n",
    "train_tab = get_tabular_data(red_sdata, X_val_train)\n",
    "print(train_tab[0])\n",
    "\n",
    "# validation set tabular data\n",
    "val_tab = get_tabular_data(red_sdata, X_val_test)\n",
    "print(val_tab[0])\n",
    "\n",
    "# test set tabular data\n",
    "test_tab = get_tabular_data(red_sdata, X_test)\n",
    "print(test_tab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and get labels in correct format for train and validation sets\n",
    "train_tok_text = tokenizer(list(X_val_train.values), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "train_labels = list(y_val_train)\n",
    "\n",
    "val_tok_text = tokenizer(list(X_val_test.values), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "val_labels = list(y_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('%d GPU(s) available' % torch.cuda.device_count())\n",
    "    print('GPU name: ', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and validation datasets\n",
    "train_dataset = get_dataset(train_tok_text, train_tab, train_labels)\n",
    "valid_dataset = get_dataset(val_tok_text, val_tab, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-brass",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', num_labels=2)\n",
    "\n",
    "bert_model = bert_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = True,\n",
    "    output_dir=\"./bioclinicalbert_tabular_model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0,\n",
    "    logging_dir=\"./bioclinicalbert_logs\",\n",
    "    save_total_limit = 2,\n",
    "    save_steps = 500,\n",
    "    eval_steps = 50,\n",
    "    logging_steps = 50,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    load_best_model_at_end= True,\n",
    "    learning_rate = 7.214289287225764e-05,\n",
    "    seed=117,\n",
    "    lr_scheduler_type = 'cosine_with_restarts',\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to directory\n",
    "trainer.save_model('./bioclinicalbert_tabular_model_tuned')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
