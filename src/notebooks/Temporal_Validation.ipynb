{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_curve, RocCurveDisplay\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import sklearn\n",
    "import os\n",
    "import shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('max_colwidth', 300)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path for importing required functions for data processing\n",
    "\n",
    "sys.path.append(\"data_processing\")\n",
    "sys.path.append(\"data_processing\")\n",
    "\n",
    "# Functions required for data processing\n",
    "\n",
    "import process_text\n",
    "import transform_textfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859fd35",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd6eaf",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05111cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractOriginalText2(input_path):\n",
    "    # read in progress note from txt file (same txt file that was imported to CLAMP for annotation)\n",
    "    results = glob.glob(f\"{input_path}/*.txt\")\n",
    "    rows = []\n",
    "    \n",
    "    # extract original note text from the progress note and save as dataframe\n",
    "    for i in range(len(results)):\n",
    "        row = pd.read_csv(results[i], sep=\"\\t\", quoting=csv.QUOTE_NONE).iloc[:, [6]]\n",
    "        row.columns = ['note_des']\n",
    "        row['file'] = os.path.basename(results[i]).split(\".\")[0]\n",
    "        rows.append(row)\n",
    "    return pd.concat(rows)\n",
    "\n",
    "def extractXMIAnnotation2(input_path):\n",
    "    # write Patterns\n",
    "    id_match = re.compile(\"(?<=xmi:id=\\\")\\d{,5}(?=\\\")\")\n",
    "    begin_match = re.compile(\"(?<=begin=\\\")\\d{,6}(?=\\\")\")\n",
    "    end_match = re.compile(\"(?<=end=\\\")\\d{,6}(?=\\\")\")\n",
    "    tag_match = re.compile(\"(?<=semanticTag=\\\")\\w*(?=\\\")\")\n",
    "\n",
    "    # read annotation file (notes were annotated in CLAMP)\n",
    "    # annotation files were exported from CLAMP in XMI format\n",
    "    results = glob.glob(f\"{input_path}/*.xmi\")\n",
    "    rows = []\n",
    "    \n",
    "    # extract annotations from XMI files (noted with semanticTag field)\n",
    "    for i in range(len(results)):\n",
    "        file = open(results[i], \"r+\")\n",
    "        lines = file.readlines()\n",
    "        lines = lines[0].split('><')\n",
    "        extract = [x for x in lines if re.search(\"semanticTag\", x)]\n",
    "        extract = [(id_match.findall(x),\n",
    "                    begin_match.findall(x),\n",
    "                    end_match.findall(x),\n",
    "                    tag_match.findall(x))\n",
    "                   for x in extract]\n",
    "        unique_tags = sorted(list(set([x[3][0] for x in extract])))\n",
    "#         file_name = re.findall(\"\\\\A.*(?=\\.)\",os.path.basename(results[i]))[0].split(\"-\")\n",
    "        file_name = os.path.basename(results[i]).split(\".\")[0]\n",
    "        \n",
    "        row = pd.DataFrame({'xmi': [os.path.basename(results[i])],\n",
    "                            'file': [os.path.basename(results[i]).split(\".\")[0]],\n",
    "                            'anon_id': [file_name.split(\"-\")[1]],\n",
    "                            'encounter_id': [file_name.split(\"-\")[2]]})\n",
    "\n",
    "        for tag in unique_tags:\n",
    "            row.loc[:, tag] = 1\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    full = pd.concat(rows).fillna(0)\n",
    "\n",
    "    # categorize and process types of ground truth annotations\n",
    "    ## PTBM: parent training in behavioral management\n",
    "    ## weak_bt: weak evidence of PTBM\n",
    "    ## strong_bt: strong evidence of PTBM\n",
    "    ## bt_yn: binary variable for any evidence of PTBM\n",
    "    full['weak_bt'] = np.where((full['Counsel_Handout_BT'] == 1) |\n",
    "                               (full['Counsel_Parent_BT'] == 1), 1, 0)\n",
    "\n",
    "    full['strong_bt'] = np.where((full['Refer_Parent_BT'] == 1) |\n",
    "                                 (full['Refer_School_BT'] == 1), 1, 0)\n",
    "\n",
    "#     full['bt_yn'] = np.where((full['weak_bt'] == 1) |\n",
    "#                              (full['strong_bt'] == 1), 1, 0)\n",
    "\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get required structured data for model in dict\n",
    "## Structure\n",
    "## tabular_data = pd.DataFrame({\n",
    "##     'dis_symp': [0, 1, 1], # 0 = disorder-level code; 1 = symptom-level code\n",
    "##     'age_35_6': [1, 0, 1], # 0 = 3-5 years old; 1 = 6 years old\n",
    "## })\n",
    "# the required variables are hard-coded into the function - make sure to change if needed\n",
    "# patient ID is set as ANON_ID\n",
    "\n",
    "def get_tabular_data(sdata, X_set):\n",
    "    tab = pd.merge(sdata, X_set, left_on='ANON_ID', right_on=X_set.index, how='right')\n",
    "    tab['dis_symp'] = tab['only.symp']\n",
    "    tab['age_35_6'] = tab['ADHD.age'].apply(lambda x: 0 if x < 6 else 1)\n",
    "    tab = tab.set_index('ANON_ID')\n",
    "    tab = tab[['dis_symp', 'age_35_6']].to_dict('records')\n",
    "    \n",
    "    return tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173bf6bc",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set metrics for comparison\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=1)\n",
    "    probabilities = torch.sigmoid(torch.tensor(p.predictions)).cpu().numpy()\n",
    "    labels = p.label_ids\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    roc_auc = roc_auc_score(labels, probabilities[:, 1])\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94584b0",
   "metadata": {},
   "source": [
    "### Temporal validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6298c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct dataset with note text, structured data, and note labels\n",
    "# different from get_dataset function because there are no labels\n",
    "\n",
    "def get_dep_dataset(text_data, tabular_data):\n",
    "    dataset = [\n",
    "        {\n",
    "            'input_ids': text_data['input_ids'][idx],\n",
    "            'attention_mask': text_data['attention_mask'][idx],\n",
    "            'tabular_data': torch.tensor(list(sample.values()), dtype=torch.float32)\n",
    "        }\n",
    "        for idx, sample in enumerate(tabular_data)\n",
    "    ]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c0b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain the prediction using threshold value for probabilities\n",
    "# used in temporal validation\n",
    "def pred_label(threshold_final, pred_prob):\n",
    "    \n",
    "    pred_label = (pred_prob >= threshold_final)\n",
    "\n",
    "    pred_label = pred_label.values.astype(int)\n",
    "    \n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-rocket",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_of_interest = \"BT_yn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull and process original text data (takes in a directory of progress note txt files)\n",
    "originalTextData = extractOriginalText2(\"cohort_2to6/Text files/combined_text\")\n",
    "print(originalTextData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull and process annotations (takes in a directory of CLAMP outputted annotation XMI files)\n",
    "annotatedXMIs = extractXMIAnnotation2(\"cohort_2to6/XMI files/combined\")\n",
    "print(annotatedXMIs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set binary outcome variable\n",
    "\n",
    "annotatedXMIs['BT_yn'] = np.where((annotatedXMIs['Counsel_Parent_BT'] == 1) | (annotatedXMIs['Counsel_Handout_BT'] == 1) | (annotatedXMIs['Refer_Parent_BT'] == 1) | (annotatedXMIs['Refer_School_BT'] == 1), 1, 0)\n",
    "annotatedXMIs['BT_yn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging data from both files \n",
    "data = originalTextData.merge(annotatedXMIs, on = \"file\", how = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function sectionize() from process_text for processing notes text data\n",
    "\n",
    "data['extractText'] = data['note_des'].apply(lambda x: process_text.sectionize(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using imported function clean_text() for processing notes text data\n",
    "\n",
    "data['extractText'] = data['extractText'].apply(lambda x: process_text.clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in structured data (not included in repository due to PHI)\n",
    "structured_data = pd.read_csv(\"bt_demographics.csv\")\n",
    "# structured_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set patient ID type to int\n",
    "data.anon_id = data.anon_id.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge text data with structured data to get complete dataset\n",
    "data = pd.merge(structured_data, data, left_on='ANON_ID', right_on='anon_id', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter down columns in dataset to those necessary for analysis:\n",
    "## text and label\n",
    "data = data.loc[:, ['extractText',label_of_interest, 'ANON_ID']]\\\n",
    "       .rename(columns = {'extractText':'text',\n",
    "                          label_of_interest: 'label'})\n",
    "\n",
    "data = data.set_index('ANON_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, 'text']\n",
    "y = data.loc[:, 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-timer",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validation, and test sets (stratified)\n",
    "## 70/30 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 117, stratify = y)\n",
    "X_val_train, X_val_test, y_val_train, y_val_test = train_test_split(X_train, y_train, test_size = 0.3, random_state = 117, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_train = pd.concat([X_val_train, y_val_train], axis = 1)\n",
    "val_test = pd.concat([X_val_test, y_val_test], axis = 1)\n",
    "test = pd.concat([X_test, y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-acting",
   "metadata": {},
   "source": [
    " #Checking the final size for train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: \", X_val_train.shape)\n",
    "print(\"X_val shape: \", X_val_test.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome distribution in train set\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome distribution in validation set\n",
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outcome distribution in test set\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-template",
   "metadata": {},
   "source": [
    "# Get Dataset for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce structured dataframe to only include the selected variables\n",
    "red_sdata = structured_data[['ANON_ID', 'ever.dis', 'only.symp', 'ADHD.age']]\n",
    "\n",
    "# train set tabular data\n",
    "train_tab = get_tabular_data(red_sdata, X_val_train)\n",
    "print(train_tab[0])\n",
    "\n",
    "# validation set tabular data\n",
    "val_tab = get_tabular_data(red_sdata, X_val_test)\n",
    "print(val_tab[0])\n",
    "\n",
    "# test set tabular data\n",
    "test_tab = get_tabular_data(red_sdata, X_test)\n",
    "print(test_tab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and get labels in correct format for train and validation sets\n",
    "train_tok_text = tokenizer(list(X_val_train.values), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "train_labels = list(y_val_train)\n",
    "\n",
    "val_tok_text = tokenizer(list(X_val_test.values), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "val_labels = list(y_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('%d GPU(s) available' % torch.cuda.device_count())\n",
    "    print('GPU name: ', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and validation datasets\n",
    "train_dataset = get_dataset(train_tok_text, train_tab, train_labels)\n",
    "valid_dataset = get_dataset(val_tok_text, val_tab, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-outdoors",
   "metadata": {},
   "source": [
    "# Temporal validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in deployment set for temporal validation\n",
    "deploy_df = pd.read_csv('cohort_2to6/notes-other-adhd-wc-visits-2021-10-19.csv')\n",
    "deploy_df = deploy_df.dropna()\n",
    "deploy_df = deploy_df.reset_index(drop=True)\n",
    "print(\"The number of samples in deployment set are:\",deploy_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-measure",
   "metadata": {},
   "source": [
    "## Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process deployment set notes\n",
    "deploy_df['extractText'] = deploy_df['note_des'].apply(lambda x: process_text.sectionize(x)[1]) \n",
    "deploy_df['extractText'] = deploy_df['extractText'].apply(lambda x: process_text.clean_text(x)) \n",
    "deploy_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter down to columns necessary for validation\n",
    "deploy_df= deploy_df.loc[:, ['ANON_ID','PAT_ENC_ID_JITTERED','extractText']]\\\n",
    "       .rename(columns = {'extractText':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The final deploy_df looks like below:\")\n",
    "deploy_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and get labels in correct format\n",
    "deploy_tok_text = tokenizer(deploy_df['text'].tolist(), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-junior",
   "metadata": {},
   "source": [
    "## Loading saved model weights and getting results for deployment set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved weights from the training for defining the model\n",
    "# load base sequence classification model\n",
    "bert_model = BertForSequenceClassification.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', num_labels=2)\n",
    "\n",
    "# load tuned model checkpoint\n",
    "checkpoint = torch.load('./bioclinicalbert_tabular_model_tuned/pytorch_model.bin')\n",
    "bert_model.load_state_dict(checkpoint)\n",
    "\n",
    "# load training arguments for tuned model checkpoint\n",
    "args = torch.load('./bioclinicalbert_tabular_model_tuned/training_args.bin')\n",
    "\n",
    "# push model to GPU\n",
    "bert_model = bert_model.to('cuda')\n",
    "\n",
    "# set trainer\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tabular data for deployment set\n",
    "deploy_tab = get_tabular_data(red_sdata, deploy_df)\n",
    "print(deploy_tab[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get deployment dataset and predictions\n",
    "deploy_dataset = get_dep_dataset(deploy_tok_text, deploy_tab)\n",
    "deploy_pred_output = trainer.predict(deploy_dataset)\n",
    "\n",
    "probabilities = deploy_pred_output.predictions\n",
    "deploy_df['probabilities'] = [x[1] for x in np.array([softmax(element) for element in probabilities])]\n",
    "deploy_pred_prob = deploy_df['probabilities']\n",
    "deploy_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold selection process in: Deployment_Val_Test_Set.ipynb\n",
    "threshold_final = 0.001152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of positively predicted cases: \")\n",
    "sum([0 if x < threshold_final else 1 for x in deploy_pred_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The deploy_df dataframe looks like below now:\")\n",
    "deploy_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing threshold and probabilities to get prediction according to threshold\n",
    "pred_prob_arr = pred_label(threshold_final, deploy_pred_prob)\n",
    "\n",
    "# updating predictions column with predictions obtained after using threshold values for probabilities\n",
    "deploy_df['predictions']= pred_prob_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Checking Results for predictions with threshold and without threshold\n",
    "\n",
    "print(\"Without Threshold:\")\n",
    "print(\"predicted 0:\", deploy_df['predictions'].tolist().count(0))\n",
    "print(\"predicted 1:\", deploy_df['predictions'].tolist().count(1))\n",
    "\n",
    "pred_prob_label_list = deploy_df['predictions'].tolist()\n",
    "\n",
    "print(\"With Threshold of %f:\" % threshold_final)\n",
    "print(\"predicted 0:\", pred_prob_label_list.count(0))\n",
    "print(\"predicted 1:\", pred_prob_label_list.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output temporal validation results to csv\n",
    "# deploy_df.to_csv('./bioclinicalbert_tabular_model_tuned/temporal_validation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_df = pd.read_csv('./bioclinicalbert_tabular_model_tuned/temporal_validation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54475c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 53 notes with positive predictions and 50 notes with negative predictions were manually annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in temporal validation annotations\n",
    "temp_ann = pd.read_csv('temp_val_annotations.csv')\n",
    "temp_ann = temp_ann[['anon_id', 'encounter_id', 'BT']]\n",
    "temp_ann = temp_ann.rename(columns={\"anon_id\": \"ANON_ID\", \n",
    "                            \"encounter_id\": \"PAT_ENC_ID_JITTERED\",\n",
    "                            \"BT\": \"manual_BT\"})\n",
    "temp_ann.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge temporal validation results with annotations\n",
    "temp_val_results = deploy_df.merge(temp_ann, left_on=\"PAT_ENC_ID_JITTERED\", right_on=\"PAT_ENC_ID_JITTERED\")\n",
    "temp_val_results = temp_val_results[['PAT_ENC_ID_JITTERED', 'probabilities', 'predictions', 'manual_BT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal validation performance metrics\n",
    "print(sklearn.metrics.classification_report(temp_val_results.manual_BT, temp_val_results.predictions))\n",
    "temp_val_results.predictions.value_counts()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
